{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3c64db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "df = pd.read_excel('./TfidfVectorizer_sklearn.xlsx')\n",
    "for i in df.columns: # loop through every column\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    print(df.loc[:, i].values)\n",
    "    X = tfidf_vectorizer.fit_transform(df.loc[:, i].values) # select column all values\n",
    "    print(tfidf_vectorizer.get_feature_names_out())\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38121725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_excel('./one_col_into_multi_cols.xlsx')\n",
    "df['data_col1'] = df['data'].apply(\n",
    "    lambda x: '(' + re.findall('(.*)\\.\\.', x)[0] + ')')\n",
    "df['data_col2'] = df['data'].apply(\n",
    "    lambda x: '(' + re.findall('\\.\\.(.*):', x)[0].split(':')[0] + ')')\n",
    "df['data_col3'] = df['data'].apply(\n",
    "    lambda x: '(' + re.findall('\\.\\w+:(.*)', x)[0] + ')')\n",
    "df.to_excel(\"one_col_into_multi_cols_new.xlsx\",\n",
    "            sheet_name='Sheet_name_1',\n",
    "            index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "extract_str = [\n",
    "    '\"tag\":\"房地产;住宅\"', '\"tag\":\"教育培训;幼儿园\"', '\"tag\":\"房地产;内部楼栋\"',\n",
    "    '\"tag\":\"交通设施;停车场\"', '\"tag\":\"购物;购物中心\"'\n",
    "]\n",
    "with open('./extract_specific_from_text.txt', 'r', encoding=\"utf8\") as f:\n",
    "    file_txt = f.read()\n",
    "    for i in extract_str:\n",
    "        tags = re.findall(i, file_txt)\n",
    "        if len(tags) > 0:\n",
    "            print(f'{list(set(tags))[0]} found in the file')\n",
    "        else:\n",
    "            print(f'{i} not found in the file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2944c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "options = Options()\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "service = ChromeService(executable_path=ChromeDriverManager().install())\n",
    "options.add_experimental_option('excludeSwitches', ['enable-logging']) # KINDLY ADD THIS OPTION\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "URL = ' https://www.aljazeera.com/economy/2023/2/6/who-is-gautam-adani-and-why-is-he-controversial'\n",
    "driver.get(URL)\n",
    "# Define your code here\n",
    "# //header[@class=\"article-header\"]/h1\n",
    "# //header[@class=\"article-header\"]//em\n",
    "# //main[@id=\"main-content-area\"]/div[2]/p[1]\n",
    "# //main[@id=\"main-content-area\"]/div[2]/p[2]\n",
    "# //main[@id=\"main-content-area\"]/div[2]/p[3]\n",
    "# //main[@id=\"main-content-area\"]/div[2]/p[4]\n",
    "h1_tag = driver.find_elements(By.XPATH, '//header[@class=\"article-header\"]/h1')[0]\n",
    "print(f'h1: {h1_tag.text}')\n",
    "em_tag = driver.find_elements(By.XPATH, '//header[@class=\"article-header\"]//em')[0]\n",
    "print(f'em: {em_tag.text}')\n",
    "for i in range(1, 5):\n",
    "    p_tag = driver.find_elements(By.XPATH, f'//main[@id=\"main-content-area\"]/div[2]/p[{i}]')[0]\n",
    "    print(f'p{i}: {p_tag.text}')\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data=[\n",
    "    {\"doctor_id\": 1, \"doctor_name\": \"Sadia\", \"doctor_gender\": \"Female\", \"Dept_visited\": \"Family Medicine\"},\n",
    "    {\"doctor_id\": 2, \"doctor_name\": \"Saad\", \"doctor_gender\": \"Male\", \"dept_visited\": \"Psychiatry\"}\n",
    "])\n",
    "df\n",
    "inputs = df.drop('doctor_name',axis='columns')\n",
    "target = df.doctor_name    \n",
    "dummies = pd.get_dummies(inputs.Dept_visited)\n",
    "dummies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5af61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "muffins = 10\n",
    "cupcakes = 10\n",
    "a = input(\"Enter the item: \")\n",
    "while a != '0' or (muffins==0 and cupcakes==0):\n",
    "    if a == 'muffin':\n",
    "        if muffins>0:\n",
    "            muffins-=1\n",
    "        else:\n",
    "            print(\"Zero muffins.\")\n",
    "    if a == 'cupcake':\n",
    "        if cupcakes>0:\n",
    "            cupcakes-=1\n",
    "        else:\n",
    "            print(\"Zero cupcakes.\")\n",
    "#     print(\"muffins: %d cupcakes: %d\" %(muffins, cupcakes))\n",
    "    a = input(\"Enter the item: \")\n",
    "\n",
    "    if muffins==0 and cupcakes==0:\n",
    "        print(\"Out of stock\")\n",
    "        break\n",
    "print(\"Today remaning muffins: %d cupcakes: %d\" %(muffins, cupcakes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f98956b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "cancer = load_breast_cancer()\n",
    "df_feat = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])\n",
    "df_target = pd.DataFrame(cancer['target'],columns=['Cancer'])\n",
    "np.ravel(df_target) # convert it into a 1-d array\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target), test_size=0.3, random_state=101)\n",
    "SVM_classifier = svm.SVC(kernel=\"rbf\", probability = True, random_state=1)\n",
    "SVM_classifier.fit(X_train, y_train)\n",
    "SVM_y_pred = SVM_classifier.predict(X_test)\n",
    "print(classification_report(y_test, SVM_y_pred))\n",
    "MLP = MLPClassifier(random_state=1, learning_rate = \"constant\", learning_rate_init=0.3, momentum = 0.2 )\n",
    "MLP.fit(X_train, y_train)\n",
    "R_y_pred = MLP.predict(X_test)\n",
    "target_names = ['No class', 'Yes Class']\n",
    "print(classification_report(y_test, R_y_pred, target_names=target_names, zero_division=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d44377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data={\n",
    "    \"text\": [\"my name is abc\", \"xyz is a fruit\", \"abc likes per\"]\n",
    "})\n",
    "lst = ['abc', 'fruit', 'likes per']\n",
    "df['terms'] = df['text'].apply(lambda x: [i for i in lst if i in x])\n",
    "df\n",
    "# L = ['abc', 'fruit', 'likes per']\n",
    "# pat = '|'.join(rf\"\\b{x}\\b\" for x in L)\n",
    "# print(pat)\n",
    "# df['text'].str.findall(pat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7617eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import datetime\n",
    "fake = Faker()\n",
    "date = fake.date()\n",
    "datetime_object = datetime.datetime.strptime(date, '%Y-%m-%d')\n",
    "datetime_object.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec622b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters, digits\n",
    "print(ascii_letters)\n",
    "print(digits)\n",
    "import random\n",
    "''.join(random.choice(ascii_letters) if i == 0 else random.choice(ascii_letters + digits) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "options = Options()\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "service = ChromeService(executable_path=ChromeDriverManager().install())\n",
    "options.add_experimental_option('excludeSwitches', ['enable-logging']) # KINDLY ADD THIS OPTION\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "URL = 'https://finance.yahoo.com/quote/AAPL/analysis?p=AAPL&guccounter=1'\n",
    "driver.get(URL)\n",
    "revenue = driver.find_element(By.XPATH, '//div[@id=\"Col1-0-AnalystLeafPage-Proxy\"]/section/table[2]/tbody')\n",
    "print(revenue.text)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9308be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('./keep_enter.xlsx')\n",
    "def replace_custom_func(x):\n",
    "    new_str = ''\n",
    "    if len(x) > 0:\n",
    "        for i in x.split('\\n'):\n",
    "            new_str += f'\"{i}\"&CHAR(10)&'\n",
    "        return \"=\" + new_str[:-10]\n",
    "    else:\n",
    "        return x\n",
    "df['Text'] = df['Text'].apply(lambda x: replace_custom_func(x))\n",
    "df.to_csv('keep_enter1.csv', sep='|', index=False)\n",
    "df = pd.read_csv('./keep_enter1.csv', sep='|')\n",
    "writer = pd.ExcelWriter('new_excel_replace12345.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# # Convert the dataframe to an XlsxWriter Excel object.\n",
    "df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "# # Get the xlsxwriter workbook and worksheet objects.\n",
    "workbook  = writer.book\n",
    "worksheet = writer.sheets['Sheet1']\n",
    "format = workbook.add_format({'text_wrap': True})\n",
    "worksheet.set_column('C:D', None, format)\n",
    "worksheet.write_formula(1, 2, df['Text'][0])\n",
    "# # Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d3bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "df = pd.DataFrame({'id': ['a', 'b', 'a', 'c', 'b', 'a'], 'date': ['d1', 'd1', 'd2', 'd2', 'd3', 'd3']})\n",
    "d = df.groupby('id').apply(lambda x: dict(zip(x.index[:2], x.index[1:])))\n",
    "d = reduce(lambda d1, d2: {**d1, **d2}, d) \n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a13fe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"stackoverflow_issue\") \\\n",
    "    .getOrCreate()\n",
    "data = [\n",
    " ([\"James\",\"Jon\",\"Jane\"]),\n",
    " ([\"Miken\",\"Mik\",\"Mike\"]),\n",
    " ([\"John\",\"Johns\"])\n",
    "]\n",
    "schema = StructType([ \n",
    "    StructField(\"Name\",ArrayType(StringType()),True)\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data=[(r,) for r in data],schema=schema)\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc637751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_datareader.data as web\n",
    "import datetime as dt\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "yf.pdr_override() # add this \n",
    "\n",
    "df = yf.download(tickers=['^GSPC'], start=\"2017-01-01\", end=\"2017-04-30\")\n",
    "# change web.DataReader to web.get_data_yahoo\n",
    "\n",
    "# fb is meta \n",
    "data = web.DataReader('META', start = dt.datetime(2012, 1, 1), end = dt.datetime(2020, 1, 1))\n",
    "print(df.head(5))\n",
    "print(\"=================================\")\n",
    "print(data.head(5))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b77bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('./sum_numbers.txt', 'r') as f:\n",
    "    # this is the line for sum all numbers in the file\n",
    "    print(sum([int(no) for no in re.findall('\\d+', f.read())])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd66cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./loop_through_2_columns.csv')\n",
    "def add_numbers(n1, n2):\n",
    "    answer = n1+n2\n",
    "    return answer\n",
    "df['result'] = df.apply(lambda x: add_numbers(x.A, x.B), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./loop_through_2_columns.csv')\n",
    "print(df.values)\n",
    "df.iloc[2, [0]] = df.iloc[2, [1]]\n",
    "print(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea174442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "df = pd.read_csv('./loop_through_2_columns.csv')\n",
    "\n",
    "df_1 = pd.DataFrame(data={\n",
    "    'Index': list(range(2, 7)),\n",
    "    'Date': [datetime.datetime(year=2011, month=6, day=22) + datetime.timedelta(minutes=i * 30) for i in range(1, 6)]\n",
    "})\n",
    "\n",
    "df['Date'] = df_1['Date']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c29f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries import\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# configure aws on command line\n",
    "# aws configure\n",
    "# enter your access key, access secret and region in which you create your bucket\n",
    "# You get your access key and access password from security credentials under the iam\n",
    "# if its disabled delete the previous one and then create the new one\n",
    "\n",
    "s3_obj = boto3.client(\"s3\") # object for accessing s3\n",
    "\n",
    "# Downloading file code\n",
    "s3_obj.download_file(\n",
    "    Filename=\"./local_file_name_which_you_download_from_s3.csv\",\n",
    "    Bucket=\"your_bucket_name\",\n",
    "    Key=\"file_in_s3_bucket.csv\"\n",
    "# )\n",
    "df = pd.read_csv('./local_file_name_which_you_download_from_s3.csv')\n",
    "df.head()\n",
    "\n",
    "# Uploading file code\n",
    "s3_obj.upload_file(\n",
    "    Filename=\"./local_file_name_which_you_download_from_s3.csv\",\n",
    "    Bucket=\"your_bucket_name\",\n",
    "    Key=\"file_in_s3_bucket.csv\"\n",
    ")\n",
    "\n",
    "s3_obj_file = s3_obj.get_object(Bucket='bucket_name', Key='file_in_s3_bucket')['Body'].read()\n",
    "df_upload = pd.read_excel(s3_obj_file)\n",
    "df_upload.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817a6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas (if not installed)\n",
    "# solution 1\n",
    "import pandas as pd\n",
    "df = pd.read_excel('./TfidfVectorizer_sklearn.xlsx')\n",
    "df\n",
    "\n",
    "# solution 2\n",
    "import openpyxl\n",
    "\n",
    "book = openpyxl.load_workbook('./TfidfVectorizer_sklearn.xlsx')\n",
    "\n",
    "sheet = book.active\n",
    "cells = sheet['A1': 'D5']\n",
    "\n",
    "for c1, c2, c3, c4 in cells:\n",
    "    print(f\"{c1.value} {c2.value} {c3.value} {c4.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca21bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# C:\\Program Files\\Google\\Chrome\\Application\n",
    "# \n",
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\Chrome.exe\"\n",
    "driver = webdriver.Chrome(service = Service(ChromeDriverManager().install()), options = options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b077fd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "df = pd.read_excel('./TfidfVectorizer_sklearn.xlsx')\n",
    "df.to_json('new_file1.json', orient='records')\n",
    "with open('./new_file1.json', 'r') as json_file:\n",
    "    a = {}\n",
    "    data = json.load(json_file)\n",
    "    a['details'] = data\n",
    "with open(\"./new_file1.json\", \"w\") as jsonFile:\n",
    "    json.dump(a, jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102ee141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def my_function():\n",
    "    for i in range(3):\n",
    "        yield(dict(x=[[random.randint(0,10)]], y=[[random.randint(0,10)]]), 0)\n",
    "a = my_function()\n",
    "print(next(a)) # one by one\n",
    "print(next(a))\n",
    "print(list(my_function())) # get all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3849c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d1 = pd.DataFrame([[1,3],[2,4]])\n",
    "print(d1)\n",
    "d2 = pd.DataFrame([['A','D'],['B','E'],['C','F']])\n",
    "print(d2)\n",
    "# Solution 1\n",
    "print(d1.merge(d2, how='cross'))\n",
    "# Solution 2\n",
    "d1['key'] = 1\n",
    "d2['key'] = 1\n",
    "d1.merge(d2, on='key').drop('key', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c6c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "empty_windows = np.empty((256, 144, 3))\n",
    "random_arr = np.random.randint(0, 100, size=(256, 144, 3)) # it's dimension should be same\n",
    "empty_windows = np.concatenate([empty_windows, random_arr], axis=2) # it can concatenate into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c9b208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "tree = ET.parse('./xml_stackoverflow.xml')\n",
    "root = tree.getroot()\n",
    "for compattrval in root.iter('CompAttrVal'):\n",
    "#     compattrval_value = int(compattrval) + 1\n",
    "#     compattrval.text = str(compattrval_value)\n",
    "    compattrval.set('name', 'updated_value')\n",
    "tree.write('output.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b5a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [{'position': 5, 'name': 'neymar'}, {'position': 3, 'name': 'ronaldo'}, {'position': 2, 'name': 'messi'}]\n",
    "sorted(lst, key=lambda x: x['position'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a479af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pandas library\n",
    "# if not pip install pandas in your command line\n",
    "import pandas as pd\n",
    "# read excel file\n",
    "df = pd.read_excel('./excel_file_modify_data.xlsx')\n",
    "# if No Subscription Name contains pt-dev create a new column Dev\n",
    "df['Dev'] = df[df['No Subscription Name'].str.contains('pt-dev')]['Cost']\n",
    "df['PPR'] = df[df['No Subscription Name'].str.contains('pt-ppr')]['Cost']\n",
    "df['PROD'] = df[df['No Subscription Name'].str.contains('pt-prod')]['Cost']\n",
    "# only add 10 % when its null\n",
    "df.loc[~(df['Dev'].isnull()), 'Dev'] = df[~(df['Dev'].isnull())]['Dev'] + df[~(df['Dev'].isnull())]['Dev'] * 0.1\n",
    "df.loc[~(df['PPR'].isnull()), 'PPR'] = df[~(df['PPR'].isnull())]['PPR'] + df[~(df['PPR'].isnull())]['PPR'] * 0.1\n",
    "df.loc[~(df['PROD'].isnull()), 'PROD'] = df[~(df['PROD'].isnull())]['PROD'] + df[~(df['PROD'].isnull())]['PROD'] * 0.1\n",
    "# fill missing values with empty string and then drop the Cost column\n",
    "df = df.fillna('').drop('Cost', axis=1)\n",
    "# write excel file with an updated dataframe\n",
    "df.to_excel('excel_file_modify_data_new.xlsx', index=False) # hope it works for yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af074e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./regex_in_csv.csv')\n",
    "d = {\n",
    "    'REPRESENTATIVE_BGR30': ''\n",
    "}\n",
    "d['REPRESENTATIVE_BGR30'] = ', '.join([str(i) for i in df.REPRESENTATIVE_BGR30])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d5ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "with open('./regex_in_csv.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    regex=r\"^REP\\w+BGR30$\"\n",
    "    d = {}\n",
    "    for row in reader:\n",
    "        for key, item in row.items():\n",
    "            if re.match(regex, key):\n",
    "                if 'REPRESENTATIVE_BGR30' in d:\n",
    "                        d['REPRESENTATIVE_BGR30'] = d['REPRESENTATIVE_BGR30'] + ', ' + item\n",
    "                else:\n",
    "                    d['REPRESENTATIVE_BGR30'] = item\n",
    "print(d)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df459bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "df = pd.read_excel('./merging_excel_datetime.xlsx')\n",
    "def createDate(x):\n",
    "    return datetime.datetime(year=int(x.Year),day=int(x.Day),month=int(x.Month)) + datetime.timedelta(hours=x.Time)\n",
    "df['Datetime'] = df.apply(lambda x: createDate(x), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "a = np.array([\n",
    "    '01/01/2021', '01/01/2022', '02/01/2021', '02/01/2022', '03/01/2020',\n",
    "    '03/01/2021', '03/01/2022', '04/01/2020', '04/01/2021', '04/01/2022',\n",
    "    '05/01/2020', '05/01/2021', '06/01/2020', '06/01/2021', '07/01/2020',\n",
    "    '07/01/2021', '08/01/2020', '08/01/2021', '09/01/2020', '09/01/2021',\n",
    "    '10/01/2020', '10/01/2021', '11/01/2020', '11/01/2021', '12/01/2020',\n",
    "    '12/01/2021'\n",
    "])\n",
    "\n",
    "# def sortDate(x):\n",
    "#     date = datetime.strptime(x, '%d/%m/%Y')\n",
    "#     return int(date.day) + int(date.month) + int(date.year)\n",
    "\n",
    "print(sorted(a, key=lambda x: datetime.strptime(x, '%d/%m/%Y')))\n",
    "# # if sort by day\n",
    "# sorted(a, key=lambda x: sortDate(x))\n",
    "def func(x):\n",
    "    x = x.split('/')\n",
    "    return '.'.join(reversed(x))\n",
    "    \n",
    "all_periods = sorted(a, key = func)\n",
    "print(all_periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed217fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(\n",
    "    data= {\n",
    "        'symbol_and_date': [\n",
    "            ('GOOG', '2023-01-18 09:30:00-05:00'), \n",
    "            ('GOOG', '2023-01-18 09:35:00-05:00'),\n",
    "            ('GOOG', '2023-01-18 09:40:00-05:00'),\n",
    "            ('GOOG', '2023-01-18 09:45:00-05:00'),\n",
    "            ('GOOG', '2023-01-18 09:50:00-05:00')\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "# df['symbol'] = df['symbol_and_date'].apply(lambda x: x[0]) \n",
    "# df['date'] = df['symbol_and_date'].apply(lambda x: x[1])\n",
    "df = pd.concat([df['symbol_and_date'].apply(pd.Series)], axis=1)\n",
    "# df.drop('symbol_and_date', axis=1, inplace=True)\n",
    "df.rename(columns={0: 'symbol', 1: 'date'}, inplace=True)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf1f6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for file in os.listdir('./cr_folder'):\n",
    "    if file.split('.')[1].lower()[:-1] == 'cr':\n",
    "        print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66038b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "for v in os.walk('./cr_folder'):\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da431a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "surl_list = []\n",
    "for i in range(1, 21):\n",
    "    surl = 'https://www.zillow.com/' + city + 'rentals/' + str(i) + '_p/'\n",
    "    surl_list.append(surl)\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\":\n",
    "    \"Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.10136\"\n",
    "}\n",
    "\n",
    "sdata_list = []\n",
    "\n",
    "for surl in surl_list:\n",
    "    soup = BeautifulSoup(\n",
    "        requests.get(surl, headers=headers).content, \"html.parser\")\n",
    "    print(soup.contents)\n",
    "#     if soup.select_one(\"script[data-zrr-shared-data-key]\") is not None:\n",
    "#         sdata = json.loads(\n",
    "#             soup.select_one(\"script[data-zrr-shared-data-key]\").contents[0].strip(\n",
    "#                     \"!<>-\"))\n",
    "#         sdata_list += sdata[\"cat1\"][\"searchResults\"][\"listResults\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9677bc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mutagen.mp3 import MP3\n",
    "from mutagen.id3 import APIC\n",
    "import requests\n",
    "\n",
    "audio = MP3('./5 Sec Intro Music No Copyright.mp3')\n",
    "\n",
    "try:\n",
    "    r = requests.get(\"https://is5-ssl.mzstatic.com/image/thumb/Music124/v4/fb/58/03/fb5803f7-071b-adc2-73d1-928a772234ac/00602547996091.rgb.jpg/300x300bb.jpg\") #this is the URL that is automatically generated by my program.\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "file_path = \"./temp-album-art.jpg\"\n",
    "with open(file_path, \"wb\") as file:\n",
    "    file.write(r.content)\n",
    "\n",
    "with open(file_path, \"rb\") as file:\n",
    "    audio.tags.add(APIC(encoding=3, \n",
    "        mime='image/jpeg', \n",
    "        type=3, \n",
    "        desc=u'Cover', \n",
    "        data=file.read()\n",
    "        ))\n",
    "    audio.save(v2_version=3) # it defaults to 4 so try to save the file with version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f1c5b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Population</th>\n",
       "      <th>New Col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>230000000</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India</td>\n",
       "      <td>100000000</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>China</td>\n",
       "      <td>400000000</td>\n",
       "      <td>33.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Country   Population    New Col\n",
       "0  Pakistan    230000000  33.333333\n",
       "1     India    100000000  33.333333\n",
       "2     China    400000000  33.333333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('countries.csv')\n",
    "df['New Col'] = 100*df['Country'].value_counts(normalize=True).values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33c51c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elements: \n",
      "{'{http://www.myschool.com/schmea/studentData}as', '{http://www.myschool.com/schmea/studentData}sourceSys', '{http://www.myschool.com/schmea/studentData}prov', '{http://www.myschool.com/schmea/studentData}acctDt', '{http://www.myschool.com/schmea/studentData}cono', '{http://www.myschool.com/schmea/studentData}stats', '{http://www.myschool.com/schmea/studentData}stuRec', '{http://www.myschool.com/schmea/studentData}ss'}\n",
      "Total: 8\n",
      "acctDt: 2023-04-04\n",
      "acctDt: 2023-05-14\n",
      "prov: AB\n",
      "prov: ON\n"
     ]
    }
   ],
   "source": [
    "# importing libraries|\n",
    "from lxml import etree\n",
    "tree = etree.parse('./xml_schema_info.xml')\n",
    "root = tree.getroot()\n",
    "ele_sets = set()\n",
    "for ele in root.xpath('.//*'):\n",
    "    ele_sets.add(ele.tag)\n",
    "print(f'elements: \\n{ele_sets}\\nTotal: {len(ele_sets)}')\n",
    "acctDt = '{http://www.myschool.com/schmea/studentData}acctDt'\n",
    "for ele in root.iter(acctDt):\n",
    "    print(f'acctDt: {ele.text}')\n",
    "prov = '{http://www.myschool.com/schmea/studentData}prov'\n",
    "for ele in root.iter(prov):\n",
    "    print(f'prov: {ele.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b7a00cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Service' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13660\\3553540072.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChromeOptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\Chrome.exe\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mService\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mChromeDriverManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_experimental_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'excludeSwitches'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'enable-logging'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# KINDLY ADD THIS OPTION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdriver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Service' is not defined"
     ]
    }
   ],
   "source": [
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = \"C:\\\\Program Files\\\\Google\\\\Chrome\\\\Application\\\\Chrome.exe\"\n",
    "driver = webdriver.Chrome(service = Service(ChromeDriverManager().install()), options = options)\n",
    "options.add_experimental_option('excludeSwitches', ['enable-logging']) # KINDLY ADD THIS OPTION\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "URL = ' https://www.facebook.com/ads/library/?active_status=all&ad_type=all&country=ES&q=inmobiliaria&search_type=keyword_unordered&media_type=all'\n",
    "driver.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c79ee957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ville</th>\n",
       "      <th>match</th>\n",
       "      <th>contains</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Paris</td>\n",
       "      <td>Talborjt,Talborjt,Ville Nouvelle</td>\n",
       "      <td>Talborjt,Ville Nouvelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rome</td>\n",
       "      <td>Hay Najah,Hay Najah,Najah</td>\n",
       "      <td>Najah,Hay Najah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ville                             match                 contains\n",
       "0  Paris  Talborjt,Talborjt,Ville Nouvelle  Talborjt,Ville Nouvelle\n",
       "1   Rome         Hay Najah,Hay Najah,Najah          Najah,Hay Najah"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ville   match\n",
    "# Paris   Talborjt,Talborjt,Ville Nouvelle\n",
    "# Rome    Hay Najah,Hay Najah,Najah\n",
    "# import pandas library\n",
    "import pandas as pd\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(data=[\n",
    "    {'Ville': 'Paris', 'match': 'Talborjt,Talborjt,Ville Nouvelle'}, \n",
    "    {'Ville': 'Rome', 'match': 'Hay Najah,Hay Najah,Najah'}\n",
    "])\n",
    "def return_unique(col):\n",
    "    # 1. it split the string ',' return a list [Talborjt,Talborjt,Ville Nouvelle]\n",
    "    # 2. convert into set to get unique {Talborjt,Ville Nouvelle}\n",
    "    # 3. join the set data to with ',' so we get Talborjt,Ville Nouvelle\n",
    "    return ','.join(set(col.split(','))) \n",
    "df['contains'] = df.apply(lambda x: return_unique(x['match']), axis=1)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
